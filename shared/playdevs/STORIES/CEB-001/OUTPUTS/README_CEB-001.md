# Cebraspe Crawler - Documenta√ß√£o T√©cnica

## 1. Objetivo e Contexto

### Objetivo
O **Cebraspe Crawler** √© um sistema automatizado para descoberta, download e cataloga√ß√£o de documentos p√∫blicos da banca Cebraspe (Centro Brasileiro de Pesquisa em Avalia√ß√£o e Sele√ß√£o e de Promo√ß√£o de Eventos). O sistema serve como base inicial para an√°lise de editais e gera√ß√£o de simulados.

### Contexto de Neg√≥cio
- **Usu√°rio**: Pipeline de ingest√£o (downstream: parser/IA)
- **Valor**: Base inicial para an√°lise de editais e gera√ß√£o de simulados
- **Dom√≠nio**: Documentos p√∫blicos (editais, provas, gabaritos, resultados)
- **Fonte**: Site oficial da Cebraspe (https://www.cebraspe.org.br)

### Funcionalidades Principais
- üîç **Descoberta Autom√°tica**: Navega√ß√£o e identifica√ß√£o de URLs de PDFs
- üì• **Download Inteligente**: Download com controle de concorr√™ncia e retry logic
- üîÑ **Deduplica√ß√£o**: Verifica√ß√£o de duplicatas por hash SHA-256
- üìö **Indexa√ß√£o**: Cataloga√ß√£o de metadados em formato JSON
- üìä **Logs Estruturados**: Logs em formato JSON para monitoramento

## 2. Como Rodar (Passo a Passo Conceitual)

### Pr√©-requisitos
- Python 3.8+
- Acesso √† internet
- Espa√ßo em disco adequado (configur√°vel)

### Instala√ß√£o
```bash
# 1. Extrair o scaffold
unzip CODE_SCAFFOLD_CEB-001.zip
cd cebraspe-crawler

# 2. Instalar depend√™ncias
pip install -r requirements.txt

# 3. Configurar ambiente
cp .env.example .env
# Editar .env com suas configura√ß√µes
```

### Execu√ß√£o B√°sica
```bash
# Execu√ß√£o simples
python src/main.py

# Modo debug
python src/main.py --debug

# Modo dry-run (sem downloads reais)
python src/main.py --dry-run
```

### Fluxo de Execu√ß√£o Conceitual

1. **Inicializa√ß√£o**
   - Carregamento de configura√ß√µes
   - Setup de logging
   - Verifica√ß√£o de diret√≥rios

2. **Descoberta de URLs**
   - Navega√ß√£o no site da Cebraspe
   - Identifica√ß√£o de links para PDFs
   - Extra√ß√£o de metadados (t√≠tulo, tipo, ano)

3. **Download de Arquivos**
   - Download ass√≠ncrono com controle de concorr√™ncia
   - Verifica√ß√£o de integridade
   - Retry em caso de falhas

4. **Deduplica√ß√£o**
   - C√°lculo de hash SHA-256
   - Verifica√ß√£o contra √≠ndice existente
   - Filtragem de duplicatas

5. **Indexa√ß√£o**
   - Atualiza√ß√£o do index.json
   - Backup do √≠ndice anterior
   - Gera√ß√£o de estat√≠sticas

6. **Finaliza√ß√£o**
   - Relat√≥rio de execu√ß√£o
   - Logs de performance
   - Limpeza de arquivos tempor√°rios

## 3. APIs (Se Houver) com Exemplos

### Configura√ß√£o Program√°tica
```python
from src.config.settings import CrawlerConfig
from src.crawler.discovery import DiscoveryEngine
from src.crawler.downloader import DownloadEngine

# Configura√ß√£o customizada
config = CrawlerConfig(
    base_url="https://www.cebraspe.org.br",
    max_concurrent_downloads=3,
    request_timeout=30,
    output_dir="./custom_data"
)

# Uso dos componentes
discovery = DiscoveryEngine(config)
downloader = DownloadEngine(config)

# Descoberta de URLs
urls = await discovery.discover_urls()

# Download de arquivos
files = await downloader.download_pdfs(urls)
```

### Consulta ao √çndice
```python
from src.storage.indexer import IndexManager

# Carregar √≠ndice
indexer = IndexManager(config)

# Buscar por tipo
editais = indexer.get_documents_by_type("edital")

# Buscar por ano
docs_2024 = indexer.get_documents_by_year(2024)

# Buscar por hash
doc = indexer.get_document_by_hash("abc123...")

# Estat√≠sticas
stats = indexer.get_stats()
```

### Modelos de Dados
```python
from src.models.document import DocumentMetadata, DocumentType

# Criar metadados
metadata = DocumentMetadata.create(
    title="Edital Concurso 2024",
    document_type=DocumentType.EDITAL,
    year=2024,
    url="https://www.cebraspe.org.br/edital.pdf",
    local_path="./data/pdfs/edital.pdf",
    file_hash="sha256_hash_here",
    file_size=2048576,
    source_domain="cebraspe.org.br"
)

# Converter para dicion√°rio
data = metadata.to_dict()
```

## 4. Vari√°veis de Ambiente e Logs

### Vari√°veis de Ambiente (.env)
```bash
# URLs e dom√≠nios
CEBRASPE_BASE_URL=https://www.cebraspe.org.br
USER_AGENT=CebraspeCrawler/1.0

# Configura√ß√µes de download
MAX_CONCURRENT_DOWNLOADS=5
REQUEST_TIMEOUT=30
RETRY_ATTEMPTS=3
RATE_LIMIT_DELAY=1.0

# Diret√≥rios
OUTPUT_DIR=./data
PDFS_DIR=./data/pdfs
LOGS_DIR=./logs

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Modo de opera√ß√£o
DEBUG=false
DRY_RUN=false
```

### Estrutura de Logs
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "logger": "src.crawler.discovery",
  "message": "Descobertas 15 URLs",
  "module": "discovery",
  "function": "discover_urls",
  "line": 45,
  "extra_fields": {
    "urls_found": 15,
    "processing_time": 2.5
  }
}
```

### N√≠veis de Log
- **DEBUG**: Informa√ß√µes detalhadas para desenvolvimento
- **INFO**: Informa√ß√µes gerais sobre o progresso
- **WARNING**: Situa√ß√µes que podem causar problemas
- **ERROR**: Erros que n√£o impedem a execu√ß√£o
- **CRITICAL**: Erros que impedem a execu√ß√£o

### Arquivos de Log
- `logs/crawler.log`: Log principal em formato JSON
- Console: Logs em formato leg√≠vel para desenvolvimento

## 5. Limita√ß√µes e Pr√≥ximos Passos

### Limita√ß√µes Atuais

#### Funcionais
- **Descoberta**: Limitada a padr√µes conhecidos de URLs
- **Tipos de Documento**: Suporte apenas para editais, provas, gabaritos e resultados
- **Formato**: Apenas arquivos PDF s√£o processados
- **Dom√≠nio**: Limitado ao site oficial da Cebraspe

#### T√©cnicas
- **Concorr√™ncia**: M√°ximo de 5 downloads simult√¢neos (configur√°vel)
- **Timeout**: 30 segundos por requisi√ß√£o (configur√°vel)
- **Armazenamento**: Apenas armazenamento local
- **Deduplica√ß√£o**: Baseada apenas em hash de arquivo

#### Operacionais
- **Monitoramento**: Logs b√°sicos, sem m√©tricas avan√ßadas
- **Recupera√ß√£o**: Backup manual do √≠ndice
- **Escalabilidade**: N√£o testado com grandes volumes

### Pr√≥ximos Passos

#### Curto Prazo (Sprint 2)
1. **Implementa√ß√£o Completa**
   - Finalizar l√≥gica de descoberta de URLs
   - Implementar sistema de download real
   - Completar deduplica√ß√£o e indexa√ß√£o

2. **Testes e Qualidade**
   - Implementar testes unit√°rios e integra√ß√£o
   - Configurar CI/CD
   - Adicionar linting e type checking

3. **Documenta√ß√£o**
   - Documentar APIs internas
   - Criar guias de troubleshooting
   - Adicionar exemplos de uso

#### M√©dio Prazo (Sprint 3-4)
1. **Melhorias de Performance**
   - Otimiza√ß√£o de algoritmos de descoberta
   - Implementa√ß√£o de cache
   - Paraleliza√ß√£o de processamento

2. **Robustez**
   - Sistema de retry mais sofisticado
   - Detec√ß√£o de mudan√ßas no site
   - Recupera√ß√£o autom√°tica de falhas

3. **Monitoramento**
   - M√©tricas de performance
   - Alertas autom√°ticos
   - Dashboard de status

#### Longo Prazo (Futuro)
1. **Escalabilidade**
   - Suporte a m√∫ltiplas fontes
   - Armazenamento em nuvem
   - Processamento distribu√≠do

2. **Intelig√™ncia**
   - ML para classifica√ß√£o de documentos
   - Detec√ß√£o autom√°tica de novos tipos
   - An√°lise de conte√∫do

3. **Integra√ß√£o**
   - APIs REST para consulta
   - Webhooks para notifica√ß√µes
   - Integra√ß√£o com sistemas downstream

### Depend√™ncias Externas
- **Site da Cebraspe**: Disponibilidade e estrutura das p√°ginas
- **Rede**: Conectividade e velocidade de internet
- **Sistema**: Espa√ßo em disco e recursos computacionais

### Riscos Identificados
1. **Mudan√ßas no Site**: Altera√ß√µes na estrutura podem quebrar a descoberta
2. **Rate Limiting**: Limita√ß√µes do servidor podem afetar performance
3. **Volume de Dados**: Crescimento pode impactar performance
4. **Manuten√ß√£o**: Necessidade de atualiza√ß√µes regulares

### Crit√©rios de Sucesso
- ‚úÖ Taxa de erro < 3%
- ‚úÖ 0 duplicatas por execu√ß√£o incremental
- ‚úÖ Performance razo√°vel (< 1 min por 100 documentos)
- ‚úÖ Logs estruturados e informativos
- ‚úÖ Cobertura de testes > 80%
- ‚úÖ Documenta√ß√£o completa e atualizada



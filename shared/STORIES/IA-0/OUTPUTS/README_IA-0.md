# README_IA-0: Infraestrutura IA ‚Äî Infer√™ncia Local com Ollama

## 1. Objetivo/Contexto

### üéØ **Objetivo**
Implementar uma infraestrutura de IA local para gera√ß√£o de quest√µes de concursos p√∫blicos, utilizando modelos LLM open-source via Ollama, garantindo custos previs√≠veis e independ√™ncia de APIs externas.

### üìã **Contexto do Projeto**
- **Projeto**: Concurso-AI Orchestrated
- **Sprint**: 2 - Camada IA ‚Äî Infra e Ingest√£o
- **Hist√≥ria**: IA-0 - Infraestrutura IA (Ollama + modelos)
- **Usu√°rio**: Motor de gera√ß√£o de quest√µes
- **Valor**: Custos previs√≠veis, independ√™ncia de API externa

### üèóÔ∏è **Arquitetura**
```mermaid
graph TD
    A[Cliente/API] --> B[Load Balancer]
    B --> C[Ollama Service]
    C --> D[Model Router]
    D --> E[Qwen2-7B]
    D --> F[Llama-3.1-8B]
    E --> G[Response Handler]
    F --> G
    G --> H[JSON Logger]
    G --> I[Cliente/API]
    
    J[Health Check] --> C
    K[Metrics Collector] --> C
    L[Config Manager] --> C
```

## 2. Como Rodar (Conceitual)

### üöÄ **Pr√©-requisitos**
- **Python 3.11+** instalado
- **Ollama** instalado e rodando
- **Modelos LLM** configurados (qwen2:7b, llama3.1:8b)
- **Porta 8000** dispon√≠vel

### üì¶ **Instala√ß√£o**
```bash
# 1. Clonar o reposit√≥rio
git clone https://github.com/g4trader/concurso-ai-orchestrated.git
cd concurso-ai-orchestrated/ia-0

# 2. Instalar depend√™ncias
make install
# ou
pip install -r requirements.txt

# 3. Configurar vari√°veis de ambiente
cp env.example .env
# Editar .env conforme necess√°rio
```

### üèÉ **Execu√ß√£o**
```bash
# Desenvolvimento
make run
# ou
uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload

# Produ√ß√£o
make run-prod
# ou
gunicorn src.main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000

# Docker
make docker-build
make docker-run
```

### üîß **Configura√ß√£o do Ollama**
```bash
# 1. Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Baixar modelos
ollama pull qwen2:7b
ollama pull llama3.1:8b

# 3. Verificar modelos
ollama list
```

## 3. APIs/Contratos com Exemplos

### üåê **Endpoints Dispon√≠veis**

#### **Health Check**
```bash
GET /api/v1/health
```

**Resposta:**
```json
{
  "status": "healthy",
  "ollama_connected": true,
  "models_available": ["qwen2:7b", "llama3.1:8b"],
  "uptime": 3600.5
}
```

#### **Listar Modelos**
```bash
GET /api/v1/models
```

**Resposta:**
```json
[
  {
    "name": "qwen2:7b",
    "size": "4.1GB",
    "modified_at": "2024-01-15T10:30:00Z",
    "digest": "sha256:abc123..."
  }
]
```

#### **Gerar Quest√£o**
```bash
POST /api/v1/generate
Content-Type: application/json

{
  "prompt": "Gere uma quest√£o sobre matem√°tica b√°sica",
  "model": "qwen2:7b",
  "temperature": 0.7,
  "max_tokens": 1000,
  "context": "Concurso p√∫blico - n√≠vel m√©dio"
}
```

**Resposta:**
```json
{
  "question": "Qual √© o resultado de 15 + 27?",
  "alternatives": [
    "A) 42",
    "B) 41", 
    "C) 43",
    "D) 40"
  ],
  "correct_answer": "A",
  "explanation": "15 + 27 = 42. A soma de 15 e 27 resulta em 42.",
  "model_used": "qwen2:7b",
  "processing_time": 1.2,
  "timestamp": "2024-01-15T10:30:00Z"
}
```

#### **Gerar em Lote**
```bash
POST /api/v1/generate/batch
Content-Type: application/json

{
  "requests": [
    {
      "prompt": "Quest√£o sobre portugu√™s",
      "model": "qwen2:7b"
    },
    {
      "prompt": "Quest√£o sobre matem√°tica", 
      "model": "llama3.1:8b"
    }
  ]
}
```

**Resposta:**
```json
{
  "results": [
    {
      "index": 0,
      "success": true,
      "data": {
        "question": "Qual √© a regra de concord√¢ncia verbal?",
        "alternatives": ["A) Sujeito", "B) Predicado", "C) Complemento", "D) Adjunto"],
        "correct_answer": "A",
        "explanation": "O verbo concorda com o sujeito...",
        "model_used": "qwen2:7b",
        "processing_time": 1.1,
        "timestamp": "2024-01-15T10:30:00Z"
      }
    },
    {
      "index": 1,
      "success": true,
      "data": {
        "question": "Quanto √© 2 + 2?",
        "alternatives": ["A) 3", "B) 4", "C) 5", "D) 6"],
        "correct_answer": "B",
        "explanation": "2 + 2 = 4",
        "model_used": "llama3.1:8b",
        "processing_time": 0.8,
        "timestamp": "2024-01-15T10:30:00Z"
      }
    }
  ]
}
```

#### **M√©tricas**
```bash
GET /api/v1/metrics
```

**Resposta:**
```json
{
  "uptime": 3600.5,
  "ollama_host": "http://localhost:11434",
  "default_model": "qwen2:7b",
  "available_models": ["qwen2:7b", "llama3.1:8b"],
  "timestamp": 1705312200.5
}
```

### üìù **Contratos de Dados**

#### **QuestionRequest**
```json
{
  "prompt": "string (obrigat√≥rio)",
  "model": "string (opcional, padr√£o: qwen2:7b)",
  "max_tokens": "integer (opcional, padr√£o: 1000)",
  "temperature": "float (opcional, padr√£o: 0.7)",
  "context": "string (opcional)"
}
```

#### **QuestionResponse**
```json
{
  "question": "string",
  "alternatives": ["string[]"],
  "correct_answer": "string",
  "explanation": "string",
  "model_used": "string",
  "processing_time": "float",
  "timestamp": "datetime"
}
```

#### **ErrorResponse**
```json
{
  "error": "string",
  "details": "object (opcional)",
  "timestamp": "datetime"
}
```

## 4. Vari√°veis de Ambiente e Logs

### üîß **Vari√°veis de Ambiente**

#### **Ollama Configuration**
```bash
OLLAMA_HOST=http://localhost:11434          # URL do Ollama
OLLAMA_TIMEOUT=30                           # Timeout em segundos
OLLAMA_MAX_RETRIES=3                        # M√°ximo de tentativas
```

#### **Model Configuration**
```bash
DEFAULT_MODEL=qwen2:7b                      # Modelo padr√£o
```

#### **API Configuration**
```bash
API_HOST=0.0.0.0                           # Host da API
API_PORT=8000                              # Porta da API
API_RELOAD=true                            # Auto-reload em desenvolvimento
```

#### **Logging**
```bash
LOG_LEVEL=INFO                             # N√≠vel de log (DEBUG, INFO, WARNING, ERROR)
```

#### **CORS**
```bash
CORS_ORIGINS=http://localhost:3000,http://localhost:8080,https://vercel.app
```

### üìä **Sistema de Logs**

#### **Formato de Log**
```json
{
  "timestamp": "2024-01-15T10:30:00.123Z",
  "level": "INFO",
  "logger": "ollama_service",
  "message": "Question generated successfully",
  "request_id": "req_123456",
  "model": "qwen2:7b",
  "processing_time": 1.2,
  "tokens_used": 150
}
```

#### **N√≠veis de Log**
- **DEBUG**: Informa√ß√µes detalhadas para debugging
- **INFO**: Informa√ß√µes gerais de opera√ß√£o
- **WARNING**: Avisos sobre situa√ß√µes an√¥malas
- **ERROR**: Erros que n√£o impedem o funcionamento
- **CRITICAL**: Erros cr√≠ticos que impedem o funcionamento

#### **Estrutura de Logs**
```
2024-01-15 10:30:00 [INFO] Starting IA-0 Ollama Service
2024-01-15 10:30:01 [INFO] Connected to Ollama at http://localhost:11434
2024-01-15 10:30:01 [INFO] Available models: ['qwen2:7b', 'llama3.1:8b']
2024-01-15 10:30:15 [INFO] Question generated successfully - model: qwen2:7b, time: 1.2s
2024-01-15 10:30:20 [WARNING] Model llama3.1:8b not responding, using fallback
2024-01-15 10:30:25 [ERROR] Ollama connection failed - retrying in 5s
```

## 5. Limita√ß√µes e Pr√≥ximos Passos

### ‚ö†Ô∏è **Limita√ß√µes Conhecidas**

#### **T√©cnicas**
- **Depend√™ncia do Ollama**: Requer Ollama rodando localmente
- **Modelos pr√©-carregados**: Modelos devem estar baixados e dispon√≠veis
- **Timeout fixo**: 30 segundos para requisi√ß√µes (n√£o configur√°vel por request)
- **M√°ximo de tokens**: 1000 tokens por gera√ß√£o (limita√ß√£o do modelo)
- **Concorr√™ncia limitada**: Performance degrada com muitas requisi√ß√µes simult√¢neas

#### **Operacionais**
- **Recursos de hardware**: Requer GPU ou CPU potente para performance adequada
- **Mem√≥ria**: Modelos consomem 4-8GB de RAM cada
- **Rede local**: N√£o funciona sem acesso √† rede local do Ollama
- **Monitoramento**: M√©tricas b√°sicas, sem alertas autom√°ticos

#### **Funcionais**
- **Formato fixo**: Quest√µes sempre em formato de m√∫ltipla escolha
- **Idioma**: Otimizado para portugu√™s brasileiro
- **Contexto limitado**: N√£o mant√©m contexto entre requisi√ß√µes
- **Valida√ß√£o b√°sica**: Valida√ß√£o m√≠nima de qualidade das quest√µes geradas

### üöÄ **Pr√≥ximos Passos**

#### **Sprint 3 - Melhorias de Performance**
- **Cache de respostas**: Implementar cache para prompts similares
- **Pool de conex√µes**: Pool de conex√µes HTTP para Ollama
- **Load balancing**: Distribui√ß√£o de carga entre modelos
- **M√©tricas avan√ßadas**: Prometheus + Grafana para monitoramento

#### **Sprint 4 - Funcionalidades Avan√ßadas**
- **Streaming**: Suporte a streaming de respostas
- **Contexto persistente**: Manter contexto entre requisi√ß√µes
- **Valida√ß√£o de qualidade**: Sistema de valida√ß√£o autom√°tica de quest√µes
- **M√∫ltiplos formatos**: Suporte a diferentes tipos de quest√£o

#### **Sprint 5 - Escalabilidade**
- **Kubernetes**: Deploy em cluster Kubernetes
- **Auto-scaling**: Escalamento autom√°tico baseado em demanda
- **Multi-region**: Deploy em m√∫ltiplas regi√µes
- **CDN**: Cache de respostas em CDN

#### **Sprint 6 - Intelig√™ncia**
- **Fine-tuning**: Fine-tuning de modelos para concursos espec√≠ficos
- **A/B testing**: Teste de diferentes modelos e par√¢metros
- **Feedback loop**: Sistema de feedback para melhorar qualidade
- **Analytics**: Analytics avan√ßados de uso e performance

### üìà **M√©tricas de Sucesso**

#### **Performance**
- **Lat√™ncia**: <5s para gera√ß√£o de quest√£o
- **Throughput**: >10 quest√µes/minuto
- **Disponibilidade**: >99% uptime
- **Tempo de resposta**: <1s para health check

#### **Qualidade**
- **Taxa de sucesso**: >95% de requisi√ß√µes bem-sucedidas
- **Qualidade das quest√µes**: >80% de quest√µes v√°lidas
- **Cobertura de testes**: >85% de cobertura de c√≥digo
- **Satisfa√ß√£o do usu√°rio**: >4.0/5.0 em avalia√ß√µes

#### **Operacionais**
- **Deploy time**: <5 minutos para deploy
- **Recovery time**: <2 minutos para recupera√ß√£o de falhas
- **Resource usage**: <80% de CPU e mem√≥ria
- **Error rate**: <1% de taxa de erro

### üîÑ **Ciclo de Melhoria Cont√≠nua**

#### **Monitoramento**
- **Logs estruturados**: An√°lise de logs para identificar padr√µes
- **M√©tricas em tempo real**: Dashboard de m√©tricas operacionais
- **Alertas proativos**: Alertas para degrada√ß√£o de performance
- **Health checks**: Verifica√ß√£o cont√≠nua de sa√∫de do sistema

#### **Feedback**
- **User feedback**: Coleta de feedback dos usu√°rios
- **Performance metrics**: An√°lise de m√©tricas de performance
- **Error analysis**: An√°lise de erros para melhorias
- **A/B testing**: Testes de diferentes abordagens

#### **Itera√ß√£o**
- **Sprints regulares**: Melhorias incrementais a cada sprint
- **Retrospectivas**: An√°lise de li√ß√µes aprendidas
- **Prioriza√ß√£o**: Prioriza√ß√£o baseada em impacto e esfor√ßo
- **Documenta√ß√£o**: Atualiza√ß√£o cont√≠nua da documenta√ß√£o

---

**Este documento fornece uma vis√£o completa da infraestrutura IA-0, incluindo objetivos, arquitetura, APIs, configura√ß√µes, limita√ß√µes e roadmap para evolu√ß√£o cont√≠nua do sistema.**

# ARCH_IA-0: Infraestrutura IA â€” InferÃªncia local com Ollama

## 1. Diagrama (entradaâ†’processamentoâ†’saÃ­da)

```mermaid
graph TD
    A[Cliente/API] --> B[Load Balancer]
    B --> C[Ollama Service]
    C --> D[Model Router]
    D --> E[Qwen2-7B]
    D --> F[Llama-3.1-8B]
    E --> G[Response Handler]
    F --> G
    G --> H[JSON Logger]
    G --> I[Cliente/API]
    
    J[Health Check] --> C
    K[Metrics Collector] --> C
    L[Config Manager] --> C
```

## 2. Pastas/arquivos a criar

```
/ia-0/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ollama_service.py
â”‚   â”‚   â”œâ”€â”€ model_router.py
â”‚   â”‚   â””â”€â”€ health_check.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ middleware.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ request.py
â”‚       â””â”€â”€ response.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_ollama_service.py
â”‚   â”œâ”€â”€ test_model_router.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md
â”‚   â””â”€â”€ DEPLOYMENT.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## 3. Contratos (schemas/DTOs) com exemplos

### Request Schema
```json
{
  "model": "qwen2-7b",
  "prompt": "Gere uma questÃ£o de portuguÃªs sobre concordÃ¢ncia verbal",
  "params": {
    "temperature": 0.7,
    "max_tokens": 1000,
    "top_p": 0.9,
    "top_k": 40
  }
}
```

### Response Schema
```json
{
  "text": "QuestÃ£o gerada...",
  "model": "qwen2-7b",
  "tokens_used": 150,
  "latency_ms": 1200,
  "timestamp": "2024-01-15T10:30:00Z"
}
```

### Error Schema
```json
{
  "error": "Model not available",
  "code": "MODEL_UNAVAILABLE",
  "details": "qwen2-7b is currently offline",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

## 4. DecisÃµes/Trade-offs

### âœ… DecisÃµes Tomadas
- **Ollama local**: Custos previsÃ­veis vs. latÃªncia de API externa
- **Dois modelos**: Qwen2-7B (portuguÃªs) + Llama-3.1-8B (geral)
- **Roteamento por modelo**: Flexibilidade vs. complexidade
- **Logs JSON**: Estruturados para observabilidade
- **Health checks**: Monitoramento proativo
- **Docker**: Facilidade de deploy vs. overhead

### âš–ï¸ Trade-offs
- **LatÃªncia**: Local (baixa) vs. Cloud (alta)
- **Custo**: Hardware prÃ³prio vs. API externa
- **ManutenÃ§Ã£o**: Self-hosted vs. managed service
- **Escalabilidade**: Vertical vs. horizontal

## 5. Checklist por etapas (P/M/G) e Riscos & MitigaÃ§Ãµes

### ğŸ”´ Prioridade Alta (P)
- [ ] Configurar Ollama local
- [ ] Implementar roteamento de modelos
- [ ] Criar endpoint `/llm/generate`
- [ ] Implementar health checks
- [ ] Configurar logs JSON
- [ ] Testes bÃ¡sicos de integraÃ§Ã£o

### ğŸŸ¡ Prioridade MÃ©dia (M)
- [ ] MÃ©tricas de performance
- [ ] Fallback entre modelos
- [ ] Rate limiting
- [ ] DocumentaÃ§Ã£o da API
- [ ] Testes de carga
- [ ] Monitoramento de recursos

### ğŸŸ¢ Prioridade Baixa (G)
- [ ] Interface de administraÃ§Ã£o
- [ ] Backup de configuraÃ§Ãµes
- [ ] OtimizaÃ§Ãµes de performance
- [ ] Dashboard de mÃ©tricas
- [ ] Alertas automÃ¡ticos
- [ ] DocumentaÃ§Ã£o avanÃ§ada

### ğŸš¨ Riscos & MitigaÃ§Ãµes

#### Risco: Modelo nÃ£o disponÃ­vel
- **Impacto**: Alto
- **Probabilidade**: MÃ©dia
- **MitigaÃ§Ã£o**: Fallback automÃ¡tico para modelo alternativo

#### Risco: Alto uso de memÃ³ria
- **Impacto**: Alto
- **Probabilidade**: Alta
- **MitigaÃ§Ã£o**: Monitoramento de recursos + restart automÃ¡tico

#### Risco: LatÃªncia alta
- **Impacto**: MÃ©dio
- **Probabilidade**: MÃ©dia
- **MitigaÃ§Ã£o**: Cache de respostas + otimizaÃ§Ã£o de prompts

#### Risco: Falha de hardware
- **Impacto**: Alto
- **Probabilidade**: Baixa
- **MitigaÃ§Ã£o**: Backup de configuraÃ§Ãµes + deploy automatizado

#### Risco: SeguranÃ§a
- **Impacto**: Alto
- **Probabilidade**: Baixa
- **MitigaÃ§Ã£o**: Sem secrets em cÃ³digo + validaÃ§Ã£o de entrada

## 6. EspecificaÃ§Ãµes TÃ©cnicas

### Endpoints
- `POST /llm/generate` - GeraÃ§Ã£o de texto
- `GET /health` - Health check
- `GET /models` - Lista modelos disponÃ­veis
- `GET /metrics` - MÃ©tricas de performance

### ConfiguraÃ§Ãµes
- `OLLAMA_HOST`: localhost:11434
- `MODEL_QWEN2`: qwen2-7b
- `MODEL_LLAMA`: llama3.1-8b
- `MAX_TOKENS`: 2000
- `TIMEOUT`: 30s
- `LOG_LEVEL`: INFO

### DependÃªncias
- Ollama (local)
- FastAPI
- Pydantic
- Uvicorn
- Prometheus client
- Python 3.9+

## 7. PrÃ³ximos Passos

1. **Implementar scaffolding** (Backend Developer)
2. **Configurar pipeline ML** (Data/ML Engineer)
3. **Criar testes** (QA Engineer)
4. **Documentar** (Technical Writer)
5. **Review de qualidade** (Reviewer)

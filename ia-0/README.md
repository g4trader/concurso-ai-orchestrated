# IA-0 Ollama Service

Servi√ßo de infer√™ncia com Ollama para gera√ß√£o de quest√µes de concursos p√∫blicos.

## üéØ Objetivo

Fornecer uma API REST para gera√ß√£o de quest√µes de concursos usando modelos LLM locais via Ollama.

## üöÄ Como rodar

### Pr√©-requisitos
- Python 3.11+
- Ollama instalado e rodando
- Modelos LLM configurados (qwen2:7b, llama3.1:8b)

### Instala√ß√£o
```bash
# Instalar depend√™ncias
make install

# Configurar vari√°veis de ambiente
cp .env.example .env
# Editar .env conforme necess√°rio
```

### Execu√ß√£o
```bash
# Desenvolvimento
make run

# Produ√ß√£o
make run-prod

# Docker
make docker-build
make docker-run
```

## üìö APIs Dispon√≠veis

### Health Check
```bash
GET /api/v1/health
```

### Listar Modelos
```bash
GET /api/v1/models
```

### Gerar Quest√£o
```bash
POST /api/v1/generate
{
  "prompt": "Gere uma quest√£o sobre matem√°tica",
  "model": "qwen2:7b",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

### Gerar em Lote
```bash
POST /api/v1/generate/batch
{
  "requests": [
    {"prompt": "Quest√£o 1"},
    {"prompt": "Quest√£o 2"}
  ]
}
```

## üîß Vari√°veis de Ambiente

- `OLLAMA_HOST`: URL do Ollama (padr√£o: http://localhost:11434)
- `OLLAMA_TIMEOUT`: Timeout para requisi√ß√µes (padr√£o: 30)
- `DEFAULT_MODEL`: Modelo padr√£o (padr√£o: qwen2:7b)
- `API_HOST`: Host da API (padr√£o: 0.0.0.0)
- `API_PORT`: Porta da API (padr√£o: 8000)
- `LOG_LEVEL`: N√≠vel de log (padr√£o: INFO)

## üß™ Testes

```bash
# Executar todos os testes
make test

# Executar com cobertura
pytest tests/ --cov=src --cov-report=html
```

## üìä Observabilidade

- Logs estruturados em JSON
- M√©tricas de performance
- Health checks
- Monitoramento de modelos

## üöÄ Deploy

### Google Cloud Run
```bash
# Construir e fazer deploy
gcloud run deploy ia-0-ollama-service \
  --source . \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

### Vercel
```bash
# Deploy autom√°tico via vercel.json
vercel --prod
```

## ‚ö†Ô∏è Limita√ß√µes Conhecidas

- Requer Ollama rodando localmente
- Modelos devem estar pr√©-carregados
- Timeout de 30s para requisi√ß√µes
- M√°ximo de 1000 tokens por gera√ß√£o

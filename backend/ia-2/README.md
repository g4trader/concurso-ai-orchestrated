# IA-2 Gera√ß√£o Condicionada

Sistema de gera√ß√£o de quest√µes com valida√ß√£o autom√°tica por banca+edital

## üéØ Objetivo

Implementar um sistema completo de gera√ß√£o de quest√µes de m√∫ltipla escolha condicionado por banca organizadora e edital, com valida√ß√£o autom√°tica de qualidade, consist√™ncia e pl√°gio.

## üöÄ Como rodar

### Pr√©-requisitos
- Python 3.11+
- Ollama instalado e rodando
- Modelos ML baixados (BGE-M3, BGE-Reranker-Large)

### Instala√ß√£o
```bash
# Instalar depend√™ncias
make install

# Configurar diret√≥rios
make setup-dirs

# Baixar modelos ML
make download-models

# Inicializar projeto completo
make init
```

### Execu√ß√£o
```bash
# Desenvolvimento
make run

# Produ√ß√£o
make run-prod

# Docker
make docker-build
make docker-run
```

## üìö APIs Dispon√≠veis

### Health Check
```bash
GET /api/v1/health
```

### Gera√ß√£o de Quest√£o
```bash
POST /api/v1/generate/question
{
  "contexts": [
    {
      "chunk_id": "chunk_001",
      "text": "Os princ√≠pios fundamentais da Constitui√ß√£o Federal...",
      "metadata": {
        "banca": "CESPE",
        "ano": 2024,
        "topico": "Direito Constitucional"
      }
    }
  ],
  "edital_summary": {
    "banca": "CESPE",
    "ano": 2024,
    "cargo": "Analista Judici√°rio",
    "topico": "Direito Constitucional",
    "dificuldade": "intermediaria",
    "estilo": "conceitual"
  },
  "topic": "Direito Constitucional"
}
```

### Gera√ß√£o em Lote
```bash
POST /api/v1/generate/batch
{
  "requests": [
    {
      "contexts": [...],
      "edital_summary": {...},
      "topic": "Direito Constitucional"
    }
  ],
  "batch_config": {
    "max_parallel": 5,
    "timeout_per_question": 30,
    "quality_threshold": 0.9
  }
}
```

### Valida√ß√£o de Quest√£o
```bash
POST /api/v1/validate/question
{
  "question": "Qual √© o princ√≠pio fundamental da Constitui√ß√£o Federal?",
  "alternatives": {
    "A": "Princ√≠pio da legalidade",
    "B": "Princ√≠pio da separa√ß√£o dos poderes",
    "C": "Princ√≠pio da igualdade"
  },
  "correct_answer": "B",
  "justification": "O princ√≠pio da separa√ß√£o dos poderes...",
  "source_chunks": ["chunk_001"]
}
```

### M√©tricas
```bash
GET /api/v1/metrics
```

## üîß Vari√°veis de Ambiente

- `LLM_MODEL`: Modelo LLM (padr√£o: llama-3.1-8b)
- `LLM_TEMPERATURE`: Temperatura do modelo (padr√£o: 0.7)
- `LLM_MAX_TOKENS`: M√°ximo de tokens (padr√£o: 2048)
- `VALIDATION_LEVEL`: N√≠vel de valida√ß√£o (strict/medium/loose)
- `CONSISTENCY_THRESHOLD`: Threshold de consist√™ncia (padr√£o: 0.8)
- `PLAGIARISM_THRESHOLD`: Threshold de pl√°gio (padr√£o: 0.3)
- `QUALITY_THRESHOLD`: Threshold de qualidade (padr√£o: 0.9)
- `API_PORT`: Porta da API (padr√£o: 8002)

## üß™ Testes

```bash
# Executar todos os testes
make test

# Executar com cobertura
pytest tests/ --cov=src --cov-report=html
```

## üìä Observabilidade

- Logs estruturados em JSON
- M√©tricas de performance
- Health checks detalhados
- Monitoramento de qualidade

## üöÄ Deploy

### Google Cloud Run
```bash
gcloud run deploy ia-2-generation \
  --source . \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

### Vercel
```bash
vercel --prod
```

## ‚ö†Ô∏è Limita√ß√µes Conhecidas

- Requer Ollama instalado e rodando
- Modelos ML consomem mem√≥ria significativa
- Gera√ß√£o de quest√µes pode ser lenta
- Valida√ß√£o rigorosa pode rejeitar quest√µes v√°lidas
